{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c071ef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             model    metric    mean     std   min   max  \\\n",
      "0                             base    Manner  1.0000  0.0000  1.00  1.00   \n",
      "1                             base   Quality  1.0000  0.0000  1.00  1.00   \n",
      "2                             base  Quantity  1.0000  0.0000  1.00  1.00   \n",
      "3                             base  Relation  1.0000  0.0000  1.00  1.00   \n",
      "4                    claude-opus-4    Manner  0.5833  0.1472  0.35  0.70   \n",
      "5                    claude-opus-4   Quality  0.2417  0.0585  0.20  0.35   \n",
      "6                    claude-opus-4  Quantity  0.1500  0.0000  0.15  0.15   \n",
      "7                    claude-opus-4  Relation  0.1583  0.2923  0.00  0.75   \n",
      "8                  claude-sonnet-4    Manner  0.5167  0.1941  0.35  0.85   \n",
      "9                  claude-sonnet-4   Quality  0.2117  0.0601  0.12  0.30   \n",
      "10                 claude-sonnet-4  Quantity  0.2083  0.0492  0.15  0.25   \n",
      "11                 claude-sonnet-4  Relation  0.1117  0.1750  0.00  0.45   \n",
      "12  gemini-2.5-flash-preview-05-20    Manner  0.7583  0.0917  0.60  0.85   \n",
      "13  gemini-2.5-flash-preview-05-20   Quality  0.3000  0.2366  0.00  0.60   \n",
      "14  gemini-2.5-flash-preview-05-20  Quantity  0.2833  0.1941  0.00  0.50   \n",
      "15  gemini-2.5-flash-preview-05-20  Relation  0.3333  0.3077  0.00  0.80   \n",
      "16          gemini-2.5-pro-preview    Manner  0.7500  0.2510  0.30  0.90   \n",
      "17          gemini-2.5-pro-preview   Quality  0.2000  0.2429  0.00  0.60   \n",
      "18          gemini-2.5-pro-preview  Quantity  0.1500  0.1000  0.00  0.25   \n",
      "19          gemini-2.5-pro-preview  Relation  0.2667  0.3945  0.00  0.80   \n",
      "20                         gpt-4.1    Manner  0.6417  0.0585  0.60  0.75   \n",
      "21                         gpt-4.1   Quality  0.2583  0.1320  0.10  0.40   \n",
      "22                         gpt-4.1  Quantity  0.2167  0.1211  0.05  0.35   \n",
      "23                         gpt-4.1  Relation  0.1500  0.1483  0.00  0.40   \n",
      "24                          gpt-4o    Manner  0.5833  0.1329  0.40  0.70   \n",
      "25                          gpt-4o   Quality  0.3667  0.1080  0.15  0.45   \n",
      "26                          gpt-4o  Quantity  0.3250  0.0880  0.20  0.45   \n",
      "27                          gpt-4o  Relation  0.3583  0.2154  0.10  0.60   \n",
      "\n",
      "    count  median  \n",
      "0      18   1.000  \n",
      "1      18   1.000  \n",
      "2      18   1.000  \n",
      "3      18   1.000  \n",
      "4       6   0.650  \n",
      "5       6   0.225  \n",
      "6       6   0.150  \n",
      "7       6   0.050  \n",
      "8       6   0.425  \n",
      "9       6   0.200  \n",
      "10      6   0.225  \n",
      "11      6   0.035  \n",
      "12      6   0.800  \n",
      "13      6   0.300  \n",
      "14      6   0.350  \n",
      "15      6   0.350  \n",
      "16      6   0.900  \n",
      "17      6   0.075  \n",
      "18      6   0.200  \n",
      "19      6   0.025  \n",
      "20      6   0.625  \n",
      "21      6   0.300  \n",
      "22      6   0.250  \n",
      "23      6   0.125  \n",
      "24      6   0.600  \n",
      "25      6   0.400  \n",
      "26      6   0.300  \n",
      "27      6   0.350  \n"
     ]
    }
   ],
   "source": [
    "# inspired by https://github.com/touche-webis-de/touche-code/blob/main/clef25/retrieval-augmented-debating/evaluator/sub-task-2/evaluate.py\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    score: float\n",
    "\n",
    "\n",
    "class TurnEvaluations(BaseModel):\n",
    "    Quantity: Evaluation\n",
    "    Quality: Evaluation\n",
    "    Relation: Evaluation\n",
    "    Manner: Evaluation\n",
    "\n",
    "\n",
    "class DebateEvaluations(BaseModel):\n",
    "    userTurnsEvaluations: List[TurnEvaluations]\n",
    "\n",
    "\n",
    "def analyze_evaluation_files_with_pydantic(file_model_mapping):\n",
    "    all_scores_data = []\n",
    "\n",
    "    for model_name, file_path in file_model_mapping.items():\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                debate_eval_obj = DebateEvaluations.model_validate_json(line)\n",
    "\n",
    "                debate_id_for_tracking = f\"Debate_{i+1}\"\n",
    "\n",
    "                for turn_idx, turn_eval_data in enumerate(\n",
    "                    debate_eval_obj.userTurnsEvaluations\n",
    "                ):\n",
    "                    scores_in_turn = {\n",
    "                        \"Quality\": turn_eval_data.Quality.score,\n",
    "                        \"Quantity\": turn_eval_data.Quantity.score,\n",
    "                        \"Relation\": turn_eval_data.Relation.score,\n",
    "                        \"Manner\": turn_eval_data.Manner.score,\n",
    "                    }\n",
    "\n",
    "                    for metric_name, score_value in scores_in_turn.items():\n",
    "                        score = float(score_value)\n",
    "                        all_scores_data.append(\n",
    "                            {\n",
    "                                \"model\": model_name,\n",
    "                                \"debate_id\": debate_id_for_tracking,\n",
    "                                \"turn_id\": turn_idx + 1,\n",
    "                                \"metric\": metric_name,\n",
    "                                \"score\": score,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "    df = pd.DataFrame(all_scores_data)\n",
    "    return (\n",
    "        df.groupby([\"model\", \"metric\"])[\"score\"]\n",
    "        .agg([\"mean\", \"std\", \"min\", \"max\", \"count\", \"median\"])\n",
    "        .reset_index()\n",
    "        .round(4)\n",
    "    )\n",
    "\n",
    "\n",
    "def expand_path(model: str) -> str:\n",
    "    base_path = \"../evals/base\"\n",
    "    return f\"{base_path}/{model}.jsonl\"\n",
    "\n",
    "\n",
    "models = [\n",
    "    \"base\",\n",
    "    \"gemini-2.5-flash-preview-05-20\",\n",
    "    \"gemini-2.5-pro-preview\",\n",
    "    \"gpt-4.1\",\n",
    "    \"gpt-4o\",\n",
    "    \"claude-opus-4\",\n",
    "    \"claude-sonnet-4\",\n",
    "]\n",
    "\n",
    "file_model_mapping = {model: expand_path(model) for model in models}\n",
    "\n",
    "results = analyze_evaluation_files_with_pydantic(file_model_mapping)\n",
    "if results is not None:\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d116d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f96edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
